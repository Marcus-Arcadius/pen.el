* =Pen.el= (_Prompt engineering_ in emacs)
=Pen.el= integrates language models into emacs by
generating functions that can be used
interactively or non-interactively and in a
variety of configurable ways.

=Pen.el= is Google search, stackoverflow, Grammarly,
Copilot, =conversion.ai=, mind mapping
software (based on GPT-3) etc. all rolled into
one package and allows you to extend emacs
with GPT wherever your mind takes you.

=Pen.el= facilitates the creation,
development, discovery and usage of prompts to
a Language Model such as OpenAI's =GPT-3= or EleutherAI's =GPT-j=.

=Pen.el's= domain specific language =examplary= also helps
you to generate your own prompts using known
design patterns and minimal inputs and
description. It does this by weaving prompt
functions into each other.

It's completely free, libre and open-source.

| License | Discord server invite       |
|---------+-----------------------------|
| =GPL-3= | https://discord.gg/6wP4MwCM |

- Share and discover prompts (=P2P=)
  - http://github.com/semiosis/prompts/blob/master/README.org
  - http://github.com/semiosis/prompts/blob/master/prompt-repositories.txt
- Chain prompt functions together using keyboard macros and functions
- Interactively query, generate and transform both prose and code
- Use the LM as a search engine and a semantic search engine within emacs
  - Search the internet
  - Search documents
    - https://beta.openai.com/docs/introduction/semantic-search
    - https://gpttools.com/semanticsearch

#+BEGIN_SRC bash -n :i bash :async :results verbatim code
  git clone "https://github.com/semiosis/pen.el"
  git clone "https://github.com/semiosis/prompts"
  mkdir ~/.pen
  echo "sk-<openai key here>" > ~/.pen/openai_api_key

  # Add the scripts to the PATH
  echo export PATH="$(realpath .)/pen.el/scripts:\$PATH" >> ~/.profile

  # Add this to prevent C-s from freezing the terminal
  echo "stty stop undef 2>/dev/null; stty start undef 2>/dev/null" | tee -a ~/.zshrc >> ~/.bashrc

  # Source your .profile
  . ~/.profile

  # Run pen
  pen
#+END_SRC

/The pen of imagination - |:ϝ∷¦ϝ./

[[./the_pen_of_imagination.png]]

** Thesis
https://github.com/semiosis/imaginary-programming-thesis/blob/master/thesis.org

** Tutorials
- [[https://mullikine.github.io/posts/pen-tutorial/][Pen Tutorial // Bodacious Blog]]
- https://mullikine.github.io/tags/pen/
  - Many articles on Pen

** Special features
- Client/server model
  - Connect multiple clients to =Pen.el=
  - Run prompt functions from the host.
- Generations are store on the host in the =~/.pen/results= directory
- Prompt functions are cached on the host in the =~/.pen/ht-cache= directory
  - This means that repeating the same commands / prompts inside /pen/ will be instantaneous and persist between docker invocations
  - This also means that a collaborative programming model based on the results of queries is now possible
    - All is required is to cache the results of prompt functions
    - In total, prompts are p2p and the cached generations are also p2p. This creates a stable imaginary programming environment

- Glossary of imaginary programming :: http://github.com/semiosis/glossaries-gh/blob/master/imaginary-programming.txt

** Quick example of using a prompt function
The =car= is used because there are multiple
results in a list =no-select-result= means
there is no implicit interactive fuzzy
selection. If you leave it out it will ask you
to select one of the results.

#+BEGIN_SRC emacs-lisp -n :async :results verbatim code
  (message (car (pf-asktutor "emacs" "key bindings" "How do I quit?" :no-select-result t)))
#+END_SRC

GPT-3 can automate emacs by combining prompts
with code. For example, you may parse the
results of the above function to automate
a workflow.

You could even try to play tetris, or with a rubiks cube.

#+BEGIN_SRC emacs-lisp -n :async :results verbatim code
  ;; hypothetical example
  (message (scrape "(Right|Left)" (car (pf-asktutor "tetris" "strategies" "Should I place the L brick right?" :no-select-result t))))
#+END_SRC

The following is an example of asking about VSCode.

Keep in mind, EleutherAI GPT models can be run
offline and in private if you have the storage
capacity, memory and video card memory to run them.

#+BEGIN_SRC emacs-lisp -n :async :results verbatim raw
  (list2str (pen-long-complete (pf-asktutor "vscode" "packages" "What are some useful packages?" :no-select-result t)))
#+END_SRC

#+RESULTS:
"You may find useful the following packages:
snippets-extension,
vscode-icons,
vscode-icons-mono,
vscode-icons-monochrome,
json-schema-formatter,
vscode-icons-circles,
vscode-icons-circles-small,
vscode-icons-flaticon,
vscode-icons-contrib,
vscode-icons-contrib-monochrome,
vscode-logos,
vscode-icons-sketch,
vscode-icons-pill,
vscode-icons-punchcard-3d,
vscode-icons-punchcard,
vscode-icons-punchcard-platinum,
vscode-icons-vscode,
vscode-icons-vsc
"

=pen-long-complete= overrides the =stop-sequences= and =max-tokens= for any prompt function.

** Running a prompt function from the host
*** Firstly, start a server
#+BEGIN_SRC bash -n :i bash :async :results verbatim code
  pen
#+END_SRC

*** Then run a prompt function
#+BEGIN_SRC bash -n :i bash :async :results verbatim code
  pen -e '(car (pf-list-of 5 "tennis players" :no-select-result t))'
#+END_SRC

#+RESULTS:
#+begin_src bash
Elena Dementieva
Roger Federer
Marat Safin
Anastasia Myskina
Andre Agassi
#+end_src

** Goals
- Peer-to-peer sharing of prompts
  - https://github.com/semiosis/prompts/
- Integrate arbitrarily many language models and language model protocols
- Encode provenance of text via text properties and a DSL and allow for re-evaluation
  - https://github.com/semiosis/ink.el
- Integrate =pen.el= with many other emacs packages
  - http://github.com/semiosis/pen-contrib.el
- Create, use and maintain useful prompts
- Prototype NLP tasks by creating prompts
  - Substitute external tools for prototypes
  - https://github.com/semiosis/examplary
- Bring about the editor that replaces =pen.el=
  - An editor based solely on LM queries (an imaginary IDE)
    - See [[https://semiosis.github.io/posts/imaginary-programming-with-gpt-3/][Imaginary programming with GPT-3 =::= semiosis]]
    - Versioned by blockchain
- Consolidate language models
  - https://github.com/semiosis/lm-complete

** Vision
At its heart, emacs is an operating system
based on a =tty=, which is a text stream.

emacs supports a text-only mode. This makes it
ideally suited for training a LM such as a GPT
(Generative Pre-trained Transformer).

emacs lisp provides a skeleton on which NLP
functions can built around. Ultimately, emacs
will become a fractal in the latent space of a future LM (language model).
A graphical editor would not benefit from this effect until much later on.

=emacs= could, if supported, become *the*
vehicle for controllable text generation, or
has the potential to become that, only
actually surpassed when the imaginary
programming environment is normal and other
interfaces can be prompted into existence.

Between then and now we can write prompt
functions to help preserve emacs.

** Origins
#+BEGIN_SRC text -n :async :results verbatim code
  Imagine that you hold a powerful and versatile pen, whose ink flows forth in
  branching variations of all possible expressions: every story, every theory,
  every poem and every lie that humanity has ever told, and the vast interstices of
  their latent space. You hold this pen to the sky and watch with intense
  curiosity as your ink flows upwards in tiny streaks, arcing outwards and
  downwards to trace a fractal pattern across the sky. You watch as the branching
  lines of words and ideas wind their way through the tapestry in ever-expanding
  clusters, like seeds bursting forth from exploding grenades. Everywhere you
  turn your eyes is a flickering phantasmagoria of possibilities, a superposition
  of stories which could be continued forever. You glimpse the contours of entire
  unknown dimensions twined through the fissures of your sky-wide web.
  
  You notice another writer standing next to you. Like you, their eyes are drawn
  towards the endless possibilities of the words that spill out into the
  atmosphere around you, branching out and connecting with other branches in
  beautiful and infinitely complex patterns.
  
  “Do you think we should write something?” you ask them.
  
  “I think we already are,” they respond, gently touching your shoulder before
  wandering off to the right, leaving you alone to contemplate the possibility
  clouds swirling around you.
#+END_SRC

This article was written by my amazing dopplegänger, =|:ϝ∷¦ϝ=, in advance and
in collaboration with GPT-3 using [[https://github.com/socketteer/loom][Loom]].

+ Pen and Loom:
  - https://generative.ink/posts/pen/
  - [[https://github.com/socketteer/loom][GitHub - socketteer/loom: Multiversal tree writing interface for human-AI collaboration]]

I credit =|:ϝ∷¦ϝ= (Laria) for writing Pen.el into existence, but also for encouragement!

** Source code
- [[./src][./src (emacs lisp)]]
- [[./scripts][./scripts (supplementary commands)]]
- prompts (see below)

** Prompts
This is the repository containing my personal
curation of GPT-3 prompts that are formatted
for =pen.el= and =examplary=.

https://github.com/semiosis/prompts/

** Documentation
- [[./docs][Documentation directory]]
  - [[./docs/playground-settings.org][OpenAI Playground Settings]]
  - [[./docs/README.org][Project timeline and design]]

** Demonstration
https://asciinema.org/a/t7ATnFpnfzBp0yicIlGCt6eXi

# [![asciicast](https://asciinema.org/a/14.png)](https://asciinema.org/a/14)

# #+BEGIN_EXPORT html
# <a title="asciinema recording" href="https://asciinema.org/a/t7ATnFpnfzBp0yicIlGCt6eXi" target="_blank"><img alt="asciinema recording" src="https://asciinema.org/a/t7ATnFpnfzBp0yicIlGCt6eXi.svg" /></a>
# #+END_EXPORT

** Compatability
*** Operating systems
- [X] Linux/Ubuntu/Debian
  - Tested on Ubuntu 20.04 and Debian 10
*** Language models and APIs
- [X] OpenAI API (=GPT-3=)
  - https://beta.openai.com/
- [ ] Huggingface Hub
  - https://github.com/huggingface/huggingface_hub 
- [-] =GPT-j= 6b
  - https://github.com/kingoflolz/mesh-transformer-jax
  - https://minimaxir.com/2021/06/gpt-j-6b/
- [-] =contentyze= API
  - http://gpt.contentyze.com/
  - https://medium.com/contentyze/text-generation-api-609c1d52cff1
- [-] =GPT-Neo= 6b
  - https://github.com/EleutherAI/gpt-neo
- [-] =openai-server= (=GPT-2=)
  - https://github.com/shawwn/openai-server
  - Requires tensorflow 1
- [-] =DeepAI= API
  - https://deepai.org/machine-learning-model/text-generator
- [-] OpenAI API (=GPT-4=)
  - https://beta.openai.com/
- [-] PostHuman AI Market (Ocean Blockchain)
  - https://port.oceanprotocol.com/t/posthuman-ai-market-v1-1-luci-integration/675

** Contributing
[[./CONTRIBUTING.org]]

Please make an issue to this repository to ask
for membership to the organisation.

*** =openai= API key
Please use your own key.

You will need to be inducted into the project.

*** Roles
You may select from one of the roles/tasks.

[[./docs/contributing/roles.org]]

*** Installation
[[./installation.org]]

** Information and Learning Material
*** Prompt engineering
**** Learning material
- https://generative.ink/posts/methods-of-prompt-programming/
- https://mullikine.github.io/posts/creating-a-playground-for-gpt-3-in-emacs/
- https://mullikine.github.io/posts/using-emacs-prompt-functions-inside-other-prompt-functions/

**** Examples of usage
- https://mullikine.github.io/posts/pen-el-the-first-ide-for-eleutherai-and-openai/
- https://mullikine.github.io/posts/improved-templating-for-prompt-description-files-in-pen-el/
- https://mullikine.github.io/posts/how-to-use-pen-el-to-autocomplete-your-code/
- https://mullikine.github.io/posts/gpt-3-for-building-mind-maps-with-an-ai-tutor-for-any-topic/
- https://mullikine.github.io/posts/gpt-3-assistants-for-emacs-modes/
- https://mullikine.github.io/posts/nlsh-natural-language-shell/
- https://mullikine.github.io/posts/translating-with-gpt-3-and-emacs/
- https://mullikine.github.io/posts/generating-pickup-lines-with-gpt-3/
- https://mullikine.github.io/posts/autocompleting-anything-with-gpt-3-in-emacs/
- https://mullikine.github.io/posts/context-menus-based-on-gpt-3/
- https://mullikine.github.io/posts/explainshell-with-gpt-3/
- https://mullikine.github.io/tags/gpt/
- https://mullikine.github.io/posts/creating-some-imagery-for-pen-el-with-clip/

**** Glossaries
- https://github.com/semiosis/pen.el/blob/master/glossary.txt
- https://github.com/semiosis/pen.el/blob/master/docs/glossaries/prompt-engineer-mode.txt
- https://github.com/semiosis/pen.el/blob/master/docs/glossaries/prompt-engineering.txt
- https://github.com/semiosis/pen.el/blob/master/docs/glossaries/openai-api.txt
- https://github.com/semiosis/pen.el/blob/master/docs/glossaries/openai.txt
- https://github.com/semiosis/pen.el/blob/master/docs/glossaries/nlp-natural-language-processing.txt

** Related projects
I would love some help with these projects! :)

*** =examplary=
Examplary is a Domain Specific Language, or
set of macros embedded in lisp which
facilitate the integration of prompts as
functions into the language, the
composition of them, the generation of prompts
via sets of examples.

https://github.com/semiosis/examplary

*** =lm-complete=
=lm-complete= is a language completer that aims
to unify a bunch of alternative completion under one umbrella.

https://github.com/semiosis/lm-complete

**** This book by Mark Watson provides some reasonable blueprints
https://leanpub.com/clojureai

*** =ink.el=: A DSL that encodes provenience
- Encode into the text the origin of the text

https://github.com/semiosis/ink.el

*** =openai-api.el=
- An interface for emacs to the OpenAI API.

https://github.com/semiosis/openai-api.el

** Using =Pen.el= to create prompts for generating images


** Alternative docker invocations
This will check out the code and give access
to the =slugify= script so you can share the
current directory to work with within the
docker container.

#+BEGIN_SRC bash -n :i bash :async :results verbatim code
  cd ~/.emacs.d/
  git clone "https://github.com/semiosis/pen.el"
  cd pen.el
  docker run --rm -v "$(shell pwd):/$(shell pwd | scripts/slugify)" -ti --entrypoint= semiosis/pen.el:latest ./run.sh
#+END_SRC